{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random\n",
    "import pathlib\n",
    "import cv2\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    print('GPU is available')\n",
    "else:\n",
    "    print('No GPU detected')\n",
    "\n",
    "num_gpus = len(physical_devices)\n",
    "\n",
    "if num_gpus > 0:\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {tf.config.experimental.get_device_details(physical_devices[0])}\")\n",
    "else:\n",
    "    print(\"No GPUs available\")\n",
    "\n",
    "device = tf.device('gpu:0' if len(physical_devices) > 0 else 'cpu:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from skimage import exposure\n",
    "\n",
    "# Directory containing MRI images\n",
    "data_dir = 'D:\\\\Brain Cancer\\\\Dataset\\\\Brain_Cancer raw MRI data\\\\Brain_Cancer'\n",
    "save_dir = 'D:\\\\Brain Cancer\\\\Dataset\\\\Brain_Cancer_processed_2'  # Directory to save preprocessed images\n",
    "\n",
    "# Create directory to save preprocessed images\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Function to apply intensity normalization (Z-score normalization)\n",
    "def normalize_image(image):\n",
    "    mean = np.mean(image)\n",
    "    std = np.std(image)\n",
    "    normalized_image = (image - mean) / std\n",
    "    return normalized_image\n",
    "\n",
    "# Function for data augmentation\n",
    "def augment_image(image):\n",
    "    aug_images = []\n",
    "\n",
    "    # Flipping horizontally\n",
    "    flipped_image = cv2.flip(image, 1)\n",
    "    aug_images.append(flipped_image)\n",
    "\n",
    "    # Adding noise\n",
    "    noise = np.random.normal(0, 0.0023**0.5, image.shape)\n",
    "    noisy_image = image + noise\n",
    "    aug_images.append(np.clip(noisy_image, 0, 1))  # Ensure pixel values are in [0, 1]\n",
    "\n",
    "    # Adjusting contrast and brightness using histogram equalization\n",
    "    contrast_image = exposure.equalize_hist(image)  # Histogram equalization to enhance contrast\n",
    "    aug_images.append(contrast_image)\n",
    "\n",
    "    # Rotations\n",
    "    rows, cols = image.shape[:2]\n",
    "    for angle in [-13, -9, +9, +13]:\n",
    "        M = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
    "        rotated_image = cv2.warpAffine(image, M, (cols, rows))\n",
    "        aug_images.append(rotated_image)\n",
    "\n",
    "    # Apply random affine transformation\n",
    "    for _ in range(2):  # Generate two affine transformations\n",
    "        pts1 = np.float32([[50, 50], [200, 50], [50, 200]])\n",
    "        pts2 = pts1 + np.random.uniform(-10, 10, size=pts1.shape).astype(np.float32)\n",
    "        M_affine = cv2.getAffineTransform(pts1, pts2)\n",
    "        affine_image = cv2.warpAffine(image, M_affine, (cols, rows))\n",
    "        aug_images.append(affine_image)\n",
    "\n",
    "    return aug_images\n",
    "\n",
    "# Loop through all subfolders and images\n",
    "for subfolder in os.listdir(data_dir):\n",
    "    subfolder_path = os.path.join(data_dir, subfolder)\n",
    "    \n",
    "    # Ensure the path is a directory\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # Create corresponding subfolder in the save directory\n",
    "        save_subfolder = os.path.join(save_dir, subfolder)\n",
    "        if not os.path.exists(save_subfolder):\n",
    "            os.makedirs(save_subfolder)\n",
    "        \n",
    "        # Loop through all images in the current subfolder\n",
    "        for img_name in tqdm(os.listdir(subfolder_path)):\n",
    "            # Read the image\n",
    "            img_path = os.path.join(subfolder_path, img_name)\n",
    "            image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load as grayscale\n",
    "            if image is None:\n",
    "                continue\n",
    "\n",
    "            # Step 1: Resize to 256x256\n",
    "            resized_image = cv2.resize(image, (256, 256))\n",
    "\n",
    "            # Step 2: Normalize (Z-score normalization)\n",
    "            normalized_image = normalize_image(resized_image)\n",
    "\n",
    "            # Step 3: Apply augmentation\n",
    "            augmented_images = augment_image(normalized_image)\n",
    "\n",
    "            # Save the processed images\n",
    "            base_name, ext = os.path.splitext(img_name)\n",
    "            cv2.imwrite(os.path.join(save_subfolder, f\"{base_name}_resized{ext}\"), (normalized_image * 255).astype(np.uint8))\n",
    "\n",
    "            # Save augmented images\n",
    "            for i, aug_image in enumerate(augmented_images):\n",
    "                cv2.imwrite(os.path.join(save_subfolder, f\"{base_name}_aug_{i}{ext}\"), (aug_image * 255).astype(np.uint8))\n",
    "\n",
    "print(f\"Preprocessing complete. Processed images saved to {save_dir}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'D:\\Brain Cancer\\Dataset\\Brain_Cancer_processed3'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "train_data = keras.utils.image_dataset_from_directory(data_dir, validation_split = 0.1, subset = 'training', seed = 1, shuffle = True, batch_size = 128, image_size=(256,256))\n",
    "\n",
    "test_data = keras.utils.image_dataset_from_directory(data_dir, validation_split = 0.1, subset = 'validation', seed = 1, shuffle = True, batch_size = 128, image_size=(256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = pathlib.Path(data_dir)\n",
    "for label in train_data.class_names :\n",
    "    images = list(filenames.glob(f'{label}/*'))\n",
    "    print(f'{label} : {len(images)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.cardinality().numpy(),  test_data.cardinality().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_data.take(152)\n",
    "val_set = train_data.skip(152)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.cardinality().numpy(), val_set.cardinality().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print random images from the train set\n",
    "plt.figure(figsize = (15, 15))\n",
    "for images, labels in train_set.take(1):\n",
    "    for i in range(15):\n",
    "        index = random.randint(0, len(images))\n",
    "        ax = plt.subplot(3, 5, i + 1)\n",
    "        plt.imshow(images[index].numpy().astype(\"uint8\"))\n",
    "        plt.title(train_data.class_names[labels[index]], color= 'blue', fontsize= 12)\n",
    "        plt.axis(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images_batch, labels_batch in train_set:\n",
    "    print(images_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Rescaling, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Flatten\n",
    "from tensorflow.keras import Model, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(256, 256, 3))\n",
    "\n",
    "# Convolutional layers with max pooling\n",
    "x = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# Flatten the output\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Fully connected layers with dropout\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(3, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.SparseCategoricalCrossentropy(), optimizer = keras.optimizers.Adam(learning_rate=1e-4), metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_1 = model.fit(train_set, epochs=30, validation_data=val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(history_df):\n",
    "    plt.figure(figsize = (13, 4), dpi = 120)\n",
    "    ax = plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, len(history_df) + 1), history_df['loss'], marker = '.', label = 'Training Loss')\n",
    "    plt.plot(range(1, len(history_df) + 1), history_df['val_loss'], marker = '^', label = 'Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    ax = plt.subplot(1, 2, 2) \n",
    "    plt.plot(range(1, len(history_df) + 1), history_df['accuracy'], marker = '.', label = 'Training Accuracy')\n",
    "    plt.plot(range(1, len(history_df) + 1), history_df['val_accuracy'], marker = '^', label = 'Validation Accurcay')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curves(pd.DataFrame(history_1.history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = None, None\n",
    "for images, labels in test_data:\n",
    "    if X_test == None or y_test == None:\n",
    "        X_test = images\n",
    "        y_test = labels\n",
    "    else:\n",
    "        X_test = tf.concat([X_test, images], axis = 0)\n",
    "        y_test = tf.concat([y_test, labels], axis = 0)\n",
    "        \n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = model.evaluate(test_data, verbose= 1)\n",
    "\n",
    "print(\"Test Loss: \", test_score[0])\n",
    "print(\"Test Accuracy: \", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['brain_glioma', 'brain_menin', 'brain_tumor']\n",
    "print(classification_report(y_test , y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,6), dpi = 100)\n",
    "sns.heatmap(metrics.confusion_matrix(y_test, y_pred), annot = True, fmt='d', cmap = 'Greens')\n",
    "plt.xlabel('Predictions')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Conusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot random images from a given dataset, and compare predictions with ground truth\n",
    "def plot_random_predictions(dataset, model):\n",
    "\n",
    "    shuffled_data = dataset.shuffle(10)\n",
    "    class_names = dataset.class_names\n",
    "\n",
    "    for images, labels in shuffled_data.take(1):\n",
    "        plt.figure(figsize = (10, 10), dpi = 120)\n",
    "        y_pred_proba = model.predict(images)\n",
    "\n",
    "    for i in range(9):\n",
    "        index = random.randint(0, len(images))\n",
    "        ax = plt.subplot(3,3, i + 1)\n",
    "\n",
    "        img = images[index].numpy().astype(\"uint8\")\n",
    "        y_true = class_names[labels[index]]\n",
    "        y_pred = class_names[np.argmax(y_pred_proba[index], axis = 0)]\n",
    "      \n",
    "        c = 'g' if y_pred == y_true else 'r'\n",
    "      \n",
    "        plt.imshow(img)\n",
    "        plt.title(f'Predicted : {y_pred}\\nTrue label : {y_true}', c = c)\n",
    "        plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_predictions(test_data, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Refat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
